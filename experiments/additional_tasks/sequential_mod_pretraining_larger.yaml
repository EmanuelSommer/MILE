saving_dir: 'results/additional_tasks/'
experiment_name: 'pretraining_seq_large'
data:
  path: imdb
  source: huggingface
  data_type: text
  task: class
  target_column: label
  features:
  - text
  datapoint_limit: 50000
  normalize: false
  train_split: 0.7
  valid_split: 0.1
  test_split: 0.2
model:
  model: AttentionClassifier
  vocab_size: 10000
  context_len: 70
  emb_size: 192
  projection_dim:
   - 128
   - 32
  n_blocks: 1
  n_heads: 10
  qkv_dim: 100
  bias: true
  dropout: 0.1
  dtype: float32
  n_classes: 2
training:
  warmstart:
    include: true
    optimizer_config:
      name: adamw
      parameters:
        learning_rate: 0.001
    warmstart_exp_dir: null
    max_epochs: 20
    batch_size: 256
    patience: 2
  sampler:
    name: 'mclmc'
    warmup_steps: 50000
    n_chains: 1
    n_samples: 10000
    use_warmup_as_init: true
    n_thinning: 100
    diagonal_preconditioning: false
    desired_energy_var_start: 0.5
    desired_energy_var_end: 0.1
    trust_in_estimate: 1.5
    num_effective_samples: 100
    step_size_init: 0.001
    keep_warmup: false
    prior_config:
      name: 'Normal'
      parameters:
        loc: 0.0
        scale: 0.4
  tokenizer:
    tokenizer: CustomBPE
    parameters: {}
logging: true
rng: 123
