saving_dir: 'results/additional_tasks/' # for the pretraining it suffices to run until the end of the warmstart phase. One can set a breakpoint in the code to stop the execution after the warmstart phase to avoid the costly sampling.
experiment_name: 'pretraining_seq'
data:
  path: imdb
  source: huggingface
  data_type: text
  task: class
  target_column: label
  features:
  - text
  datapoint_limit: 50000
  normalize: false
  train_split: 0.7
  valid_split: 0.1
  test_split: 0.2
model:
  model: AttentionClassifier
  vocab_size: 10000
  context_len: 70
  emb_size: 192
  n_blocks: 1
  n_heads: 8
  qkv_dim: 64
  bias: true
  dropout: 0.1
  dtype: float32
  n_classes: 2
training:
  warmstart:
    include: true
    optimizer_config:
      name: adamw
      parameters:
        learning_rate: 0.01
        weight_decay: 0.001
    warmstart_exp_dir: null
    max_epochs: 2
    batch_size: 256
    patience: 2
  sampler:
    name: 'mclmc'
    warmup_steps: 5000
    n_chains: 1
    n_samples: 1000
    use_warmup_as_init: true
    n_thinning: 100
    diagonal_preconditioning: false
    desired_energy_var_start: 0.5
    desired_energy_var_end: 0.1
    trust_in_estimate: 1.5
    num_effective_samples: 100
    step_size_init: 0.001
    keep_warmup: false
    prior_config:
      name: 'Normal'
      parameters:
        loc: 0.0
        scale: 0.2
  tokenizer:
    tokenizer: CustomBPE
    parameters: {}
logging: true
rng: 123
